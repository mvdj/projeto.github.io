{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TiposDeVinho-RedesNeuraisArtificiais.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4dQOBp5zTJIrz1gsLGXj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdj/mvdj.github.io/blob/master/TiposDeVinho_RedesNeuraisArtificiais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIwbbppWWH68",
        "colab_type": "text"
      },
      "source": [
        "# **Tipos de Vinho - Classificação com Redes Neurais Artificiais (RNA)**\n",
        "\n",
        "Dataset (https://archive.ics.uci.edu/ml/datasets/wine+quality) com 8096 instâncias de dados sobre os tipos de vinhos (Red and White). Dados descrevem componentes químicos/biológicos que fazem parte da qualidade de um vinho.\n",
        "\n",
        "* Aprendizado supervisionado\n",
        "* Mapeamento de um vetor de atributos para um atributo de classe\n",
        "* Seja $x_i$ um conjunto de $n$ instâncias pertencentes a uma classe $c$ \n",
        "  * $x_i$ tem dimensão $d$\n",
        "  * existem $m$ classes, $c \\in {c_1,...,c_m}$\n",
        "* Aprendizagem é identificar a função $f$ tal que:\n",
        "  * $f([x_{i1},x_{i2},...,x_{id}]) = c$\n",
        "* O aprendizado em uma RNA consiste no ajuste dos pesos\n",
        "  * a minimização do erro é a função objetivo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXpUIVgBxVtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fb1a75c6-954a-48ab-ca92-7bfa7a7965b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPmNEB4Tet_0",
        "colab_type": "text"
      },
      "source": [
        "# **1. Importação das bibliotecas**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8S1mD-4ezYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando bibliotecas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLAN7-gke52U",
        "colab_type": "text"
      },
      "source": [
        "# **2. Lendo dados**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir6sZNcNfLV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lendo csv e armazenando em um dataframe\n",
        "dados = pd.read_csv('/content/drive/My Drive/Colab Notebooks/dataSets/QualidadeVinho/winequality-whiteAndRed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q3GlbPCe7Yk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "34096414-d544-48c3-afb7-7ef1d1458a4d"
      },
      "source": [
        "# verificando o dataframe\n",
        "dados.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wine type</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>white</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>45.00</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1001.0000</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>49.00</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>994.0000</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  wine type  fixed acidity  volatile acidity  ...  sulphates  alcohol  quality\n",
              "0     white            7.0              0.27  ...       0.45      8.8        6\n",
              "1     white            6.3              0.30  ...       0.49      9.5        6\n",
              "2     white            8.1              0.28  ...       0.44     10.1        6\n",
              "3     white            7.2              0.23  ...       0.40      9.9        6\n",
              "4     white            7.2              0.23  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPQQszHriJRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f15fca0-9672-4b6b-9d97-9632197c3237"
      },
      "source": [
        "#verificando colunas dos dados\n",
        "dados.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['wine type', 'fixed acidity', 'volatile acidity', 'citric acid',\n",
              "       'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
              "       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n",
              "       'quality'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OtGJFSenp_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed3f77d2-1457-41e2-9b1b-e75eeac322e0"
      },
      "source": [
        "#verificar a quantidade de instâncias\n",
        "dados.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8096, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s60w6UsZiFMD",
        "colab_type": "text"
      },
      "source": [
        "# **3. Limpeza e organização dos dados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfBHj-r9iXHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#verificar e eliminar se existem valores NAN, ? e/ou faltantes\n",
        "dados = dados.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4WLooWiOQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eliminar colunas irrelevantes para o contexto\n",
        "dados = dados.drop(columns=['density']) # atributo com valores desproporcional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY-8Z-XVn1uG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "062e04b0-970a-4405-8d33-ef3d7b4bd82a"
      },
      "source": [
        "#verificar quantidade de instâncias sem valores NAN, ? e/ou faltantes\n",
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wine type</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>white</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>45.00</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>white</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>49.00</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>white</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>white</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  wine type  fixed acidity  volatile acidity  ...  sulphates  alcohol  quality\n",
              "0     white            7.0              0.27  ...       0.45      8.8        6\n",
              "1     white            6.3              0.30  ...       0.49      9.5        6\n",
              "2     white            8.1              0.28  ...       0.44     10.1        6\n",
              "3     white            7.2              0.23  ...       0.40      9.9        6\n",
              "4     white            7.2              0.23  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uRtI1Oti7Ap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "243006b0-291b-4733-f320-e871bcf43278"
      },
      "source": [
        "#trocando o tipo do atributo 'wine type'  por um tipo numerico\n",
        "dados['wine type'] = dados['wine type'].replace(['white','red'],[0,1]) # 0 - para white wine | 1 - para red wine\n",
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wine type</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>45.00</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>49.00</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>58.00</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   wine type  fixed acidity  volatile acidity  ...  sulphates  alcohol  quality\n",
              "0          0            7.0              0.27  ...       0.45      8.8        6\n",
              "1          0            6.3              0.30  ...       0.49      9.5        6\n",
              "2          0            8.1              0.28  ...       0.44     10.1        6\n",
              "3          0            7.2              0.23  ...       0.40      9.9        6\n",
              "4          0            7.2              0.23  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8rlrEDkjRRZ",
        "colab_type": "text"
      },
      "source": [
        "# **4. Re-escala dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-553UHO0mW",
        "colab_type": "text"
      },
      "source": [
        "**Re-escala usando máximo e mínimo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DEr7uvrO6Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dados = (dados - dados.min())/(dados.max()-dados.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMBnFTq2jYo_",
        "colab_type": "text"
      },
      "source": [
        "# **5. Organizando dados para modelagem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFMrHxWyPag-",
        "colab_type": "text"
      },
      "source": [
        "**Dividir os dados em atributos descritores e atributo de classe (target)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4oyW0bjf9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "24c50552-91c6-40cf-c9a7-ca1c1874aca7"
      },
      "source": [
        "#dividindo dados em atributos descritores e atributo de classe\n",
        "X = dados.iloc[:,1:]\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.264463</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.216867</td>\n",
              "      <td>0.308282</td>\n",
              "      <td>0.073619</td>\n",
              "      <td>0.152778</td>\n",
              "      <td>0.377880</td>\n",
              "      <td>0.217054</td>\n",
              "      <td>0.129213</td>\n",
              "      <td>0.115942</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.206612</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.204819</td>\n",
              "      <td>0.015337</td>\n",
              "      <td>0.080166</td>\n",
              "      <td>0.045139</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.449612</td>\n",
              "      <td>0.151685</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.355372</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.240964</td>\n",
              "      <td>0.096626</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.100694</td>\n",
              "      <td>0.209677</td>\n",
              "      <td>0.418605</td>\n",
              "      <td>0.123596</td>\n",
              "      <td>0.304348</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.192771</td>\n",
              "      <td>0.121166</td>\n",
              "      <td>0.094897</td>\n",
              "      <td>0.159722</td>\n",
              "      <td>0.414747</td>\n",
              "      <td>0.364341</td>\n",
              "      <td>0.101124</td>\n",
              "      <td>0.275362</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.192771</td>\n",
              "      <td>0.121166</td>\n",
              "      <td>0.094897</td>\n",
              "      <td>0.159722</td>\n",
              "      <td>0.414747</td>\n",
              "      <td>0.364341</td>\n",
              "      <td>0.101124</td>\n",
              "      <td>0.275362</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates   alcohol  quality\n",
              "0       0.264463          0.000160     0.216867  ...   0.129213  0.115942      0.5\n",
              "1       0.206612          0.000186     0.204819  ...   0.151685  0.217391      0.5\n",
              "2       0.355372          0.000169     0.240964  ...   0.123596  0.304348      0.5\n",
              "3       0.280992          0.000127     0.192771  ...   0.101124  0.275362      0.5\n",
              "4       0.280992          0.000127     0.192771  ...   0.101124  0.275362      0.5\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqk0Y-hhj4z_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "47b2d8e9-59b8-4768-d0e8-5838fb9f7f3e"
      },
      "source": [
        "y = dados['wine type']\n",
        "y.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.0\n",
              "1    0.0\n",
              "2    0.0\n",
              "3    0.0\n",
              "4    0.0\n",
              "Name: wine type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY2x_NLHLpRO",
        "colab_type": "text"
      },
      "source": [
        "**Dividir os dados em treino e teste**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uh5qdSL6Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4alA7NDjQPXH",
        "colab_type": "text"
      },
      "source": [
        "* Divide a matriz em subconjuntos aleatórios de treino e teste\n",
        "  * test_size: tamanho do subconjunto de teste (em percentual)\n",
        "  * random_state: define a semente para a aleatoriedade (se não definido, semente aleatória)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWyps5E0QSaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)#random_state=42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzKB6Gj-QTgQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d356fcd3-c9b9-4cdf-d9b0-3307600b26b5"
      },
      "source": [
        "X_train.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5138</th>\n",
              "      <td>0.421488</td>\n",
              "      <td>0.535834</td>\n",
              "      <td>0.222892</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.430423</td>\n",
              "      <td>0.013889</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.217054</td>\n",
              "      <td>0.488764</td>\n",
              "      <td>0.188406</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061</th>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.662424</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.127631</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.034562</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.252809</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2459</th>\n",
              "      <td>0.231405</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.236963</td>\n",
              "      <td>0.073619</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.324885</td>\n",
              "      <td>0.279070</td>\n",
              "      <td>0.168539</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6023</th>\n",
              "      <td>0.413223</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.210843</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.089987</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.207865</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6347</th>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.186747</td>\n",
              "      <td>0.021472</td>\n",
              "      <td>0.091623</td>\n",
              "      <td>0.048611</td>\n",
              "      <td>0.052995</td>\n",
              "      <td>0.395349</td>\n",
              "      <td>0.303371</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  ...   alcohol   quality\n",
              "5138       0.421488          0.535834  ...  0.188406  0.333333\n",
              "6061       0.429752          0.662424  ...  0.289855  0.333333\n",
              "2459       0.231405          0.000211  ...  0.173913  0.333333\n",
              "6023       0.413223          0.000135  ...  0.478261  0.666667\n",
              "6347       0.280992          0.000253  ...  0.478261  0.833333\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYbHpZU37opo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28cc199a-134f-4dbd-a583-0e3c4113c092"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5667, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mh1F0bH7rv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e3f80a6-f5ee-40d9-9ec6-fe8f1d62e0c6"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2429, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaVErfoOQYbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e4d8c513-e689-4925-8ca9-a6efed649da8"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5138    0.0\n",
              "6061    0.0\n",
              "2459    0.0\n",
              "6023    0.0\n",
              "6347    0.0\n",
              "Name: wine type, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au_KB1V6g_Xn",
        "colab_type": "text"
      },
      "source": [
        "**Comentários**\n",
        "\n",
        "*Até então, no tópico acima, foi realizada a divisão/partição dos dados em treino(70%)/teste(30%).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKZ77JVDL-dp",
        "colab_type": "text"
      },
      "source": [
        "# **5. Definindo algoritmo de aprendizado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZAFsHkRQlrg",
        "colab_type": "text"
      },
      "source": [
        "Rede Neural Multi-Layer Perceptron (MLP)\n",
        "\n",
        "Parâmetros da MLP:\n",
        "* Número de neurônios e camadas - hidden_layer_sizes\n",
        "  * tupla com a arquitetura\n",
        "  * ex: (100,10) - duas camadas escondidas com 100 e 10 neurônios respectivamente\n",
        "  * ex: (100,50,10)\n",
        "* Função de ativação - activation\n",
        "  * função de ativação das camadas escondidas\n",
        "  * identidade - identity\n",
        "  * sigmóide logística - logistic\n",
        "  * tangente hiperbólica - tanh\n",
        "  * função de unidade linear retificada - relu (max(0,x))\n",
        "* Treinamento - solver\n",
        "  * forma de otimizar os pesos da rede\n",
        "  * gradiente estocástico  proposto por Kingma, Diederik, and Jimmy Ba - adam\n",
        "  * descida do gradiente estocástico - sgd\n",
        "  * familia dos métodos quasi-Newton - lbfgs\n",
        "* Taxa de aprendizado - learning_rate\n",
        "  * taxa constant - constant\n",
        "  * decrescente - invscaling\n",
        "  * adaptativa - adaptive\n",
        "* Número máximo de iterações - max_iter\n",
        "  * número de épocas de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMZahM_YQo-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhlumus4Qspi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#definindo modelo\n",
        "classificador = MLPClassifier(hidden_layer_sizes=(100),activation='logistic',max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEgKArmfQu_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fd412a8b-f07b-4136-d8e2-84a9ae386a1d"
      },
      "source": [
        "#treinando modelo\n",
        "classificador.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
              "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=100, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ5UJvxKKDnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "3a8075b1-280a-407a-e36e-da4f0cdd09e3"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5138</th>\n",
              "      <td>0.421488</td>\n",
              "      <td>0.535834</td>\n",
              "      <td>0.222892</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.430423</td>\n",
              "      <td>0.013889</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.217054</td>\n",
              "      <td>0.488764</td>\n",
              "      <td>0.188406</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061</th>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.662424</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.127631</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.034562</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.252809</td>\n",
              "      <td>0.289855</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2459</th>\n",
              "      <td>0.231405</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.236963</td>\n",
              "      <td>0.073619</td>\n",
              "      <td>0.104167</td>\n",
              "      <td>0.324885</td>\n",
              "      <td>0.279070</td>\n",
              "      <td>0.168539</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6023</th>\n",
              "      <td>0.413223</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.210843</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.089987</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.207865</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6347</th>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.186747</td>\n",
              "      <td>0.021472</td>\n",
              "      <td>0.091623</td>\n",
              "      <td>0.048611</td>\n",
              "      <td>0.052995</td>\n",
              "      <td>0.395349</td>\n",
              "      <td>0.303371</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6596</th>\n",
              "      <td>0.355372</td>\n",
              "      <td>0.459879</td>\n",
              "      <td>0.108434</td>\n",
              "      <td>0.019939</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.066820</td>\n",
              "      <td>0.449612</td>\n",
              "      <td>0.207865</td>\n",
              "      <td>0.144928</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>0.289256</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.445783</td>\n",
              "      <td>0.197853</td>\n",
              "      <td>0.063799</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.366359</td>\n",
              "      <td>0.232558</td>\n",
              "      <td>0.101124</td>\n",
              "      <td>0.101449</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1612</th>\n",
              "      <td>0.330579</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.295181</td>\n",
              "      <td>0.039877</td>\n",
              "      <td>0.044159</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.186636</td>\n",
              "      <td>0.240310</td>\n",
              "      <td>0.056180</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5002</th>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.013889</td>\n",
              "      <td>0.069124</td>\n",
              "      <td>0.472868</td>\n",
              "      <td>0.146067</td>\n",
              "      <td>0.202899</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7816</th>\n",
              "      <td>0.438017</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.409639</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.677567</td>\n",
              "      <td>0.059028</td>\n",
              "      <td>0.133641</td>\n",
              "      <td>0.139535</td>\n",
              "      <td>0.623596</td>\n",
              "      <td>0.159420</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5667 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  ...   alcohol   quality\n",
              "5138       0.421488          0.535834  ...  0.188406  0.333333\n",
              "6061       0.429752          0.662424  ...  0.289855  0.333333\n",
              "2459       0.231405          0.000211  ...  0.173913  0.333333\n",
              "6023       0.413223          0.000135  ...  0.478261  0.666667\n",
              "6347       0.280992          0.000253  ...  0.478261  0.833333\n",
              "...             ...               ...  ...       ...       ...\n",
              "6596       0.355372          0.459879  ...  0.144928  0.500000\n",
              "1588       0.289256          0.000186  ...  0.101449  0.333333\n",
              "1612       0.330579          0.000152  ...  0.478261  0.666667\n",
              "5002       0.280992          0.000346  ...  0.202899  0.333333\n",
              "7816       0.438017          0.000574  ...  0.159420  0.500000\n",
              "\n",
              "[5667 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rQBYeAYKNvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ece95077-895d-4610-9147-778f71db0ac7"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5138    0.0\n",
              "6061    0.0\n",
              "2459    0.0\n",
              "6023    0.0\n",
              "6347    0.0\n",
              "       ... \n",
              "6596    1.0\n",
              "1588    0.0\n",
              "1612    0.0\n",
              "5002    0.0\n",
              "7816    1.0\n",
              "Name: wine type, Length: 5667, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr21wvXKf_XR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdd1eb28-ad70-41fa-f4a4-0f3911c77ee5"
      },
      "source": [
        "#realizando classificação\n",
        "classificacao = classificador.predict(X_test)\n",
        "classificacao"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., ..., 1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jkZsZShmcv",
        "colab_type": "text"
      },
      "source": [
        "**Comentários**\n",
        "\n",
        "*No tópico acima, foi trabalhado o modelo do algoritmo MLP, com 1 camada escondida de 100 neurônios, com ativação dos pesos em sigmóide (com valores para o peso entre 0 e 1) e com 100 interações de épocas.*\n",
        "\n",
        "*Foi realizado também a predição dos dados de text.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t36BaSQYMXg5",
        "colab_type": "text"
      },
      "source": [
        "# **6. Avaliação do classificador**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN7GcGxiT7JM",
        "colab_type": "text"
      },
      "source": [
        "Acurácia\n",
        "* taxa de acertos do classificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FGAgW4mVprH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculando acurácia\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caje6rYwVuaY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4524ab8-d337-4f36-9817-6bd81748ca0b"
      },
      "source": [
        "acuracia = accuracy_score(y_test,classificacao)\n",
        "round(acuracia,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.794"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv2746gE1QYy",
        "colab_type": "text"
      },
      "source": [
        "Precisão\n",
        "* taxa de instâncias classificadas como positivas que são realmente positivas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5_G0aDVvI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculando precisão\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGZPb7ymVxi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93ac8fca-3bda-448c-d32d-360560a438f5"
      },
      "source": [
        "precisao = precision_score(y_test,classificacao)\n",
        "round(precisao,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.452"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ccN1fzi1YJc",
        "colab_type": "text"
      },
      "source": [
        "Recall\n",
        "* taxa de instâncias positivas classificadas corretamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adIRL4YVzgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculando recall (revocação)\n",
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lct5nTWXV8sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3514e71-0be1-4042-cfab-693a24e349ff"
      },
      "source": [
        "recall = recall_score(y_test,classificacao)\n",
        "round(recall,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hNyMDIr1l43",
        "colab_type": "text"
      },
      "source": [
        "F1-score\n",
        "* balanço entre precisão e recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_921bIWWBIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculando f1-score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDtG-SiLWB_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35c174da-0902-4049-8008-8a7eb16373b5"
      },
      "source": [
        "f1 = f1_score(y_test,classificacao)\n",
        "round(f1,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6cPfgYDWIjn",
        "colab_type": "text"
      },
      "source": [
        "### Curva ROC\n",
        "* Representação gráfica do desempenho de um classificador binário\n",
        "* Razão entre a taxa de positivos verdadeiros (TPR) e positivos falsos (FPR)\n",
        "  * $tpr = \\dfrac{tp}{tp+fn} = \\dfrac{positivos\\_verdadeiros}{positivos\\_totais}$ \n",
        "    * (recall)\n",
        "  * $fpr = \\dfrac{fp}{tn+fp} = \\dfrac{positivos\\_falsos}{negativos\\_totais}$\n",
        "* Interpretação\n",
        "  * quanto maior tpr, melhor\n",
        "  * quanto menor fpr, melhor\n",
        "\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/3/36/ROC_space-2.png width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYnZwk4xWNfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotando curva roc\n",
        "from sklearn.metrics  import roc_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNjVE-cZWRyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_test,classificacao)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeG1vMPPWXwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "52cbee69-5359-4f46-cdc1-738e10dac6c6"
      },
      "source": [
        "plt.plot(fpr,tpr,marker='.')\n",
        "plt.title('Curva ROC')\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiro Positivos')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6FSz4XlWu71",
        "colab_type": "text"
      },
      "source": [
        "## Área sob a curva (*Area under the curve - AUC)*\n",
        "* Área sob a curva ROC\n",
        "* Interpretação numérica da curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MPmzd0KXH9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculando area sob a curva ROC\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy9KtyZpXMAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e2951cb-d4e4-492c-c59d-56e7b5a2f973"
      },
      "source": [
        "erro = roc_auc_score(y_test,classificacao)\n",
        "round(erro,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6CSrznqXY-u",
        "colab_type": "text"
      },
      "source": [
        "## **Validação cruzada**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgDXG22yXp3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# avaliando modelo com cross validation\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmP2QdinXr0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define modelo\n",
        "classificador = MLPClassifier(hidden_layer_sizes=(100),activation='logistic',max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwZwY15EXw-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0623d095-c4bb-46df-f6a8-0987ca974111"
      },
      "source": [
        "#calculando os scores\n",
        "scores = cross_val_score(classificador,X,y,cv=10)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.85679012, 0.88395062, 0.88148148, 0.87654321, 0.87654321,\n",
              "       0.85925926, 0.83559951, 0.67490729, 0.29295426, 0.36093943])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrz1qHmwX6p6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6746019c-34dd-4b81-98a5-0155920ed635"
      },
      "source": [
        "round(scores.mean(),3),round(scores.std(),3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.74, 0.215)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frZS-FUlYQ9G",
        "colab_type": "text"
      },
      "source": [
        "# **7. Comparando MLP com Árvore de Decisão e Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upeSXHRjYcZu",
        "colab_type": "text"
      },
      "source": [
        "### Validação Cruzada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIUQBRU9YgeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le4Ibc-JYhbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#criando árvore\n",
        "arvore = DecisionTreeClassifier()\n",
        "\n",
        "#calculando os scores\n",
        "scores_arvore = cross_val_score(arvore,X,y,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-FRVb9SYoGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#criando random forest\n",
        "floresta = RandomForestClassifier()\n",
        "\n",
        "#calculando os scores\n",
        "scores_floresta = cross_val_score(floresta,X,y,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lObuLxBlYuUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#criando rede neural\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100),activation='logistic',max_iter=1000)\n",
        "\n",
        "#calculando os scores\n",
        "scores_mlp = cross_val_score(mlp,X,y,cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hKdOGEzZACD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "279d42c8-787e-46a3-be5a-f9aa28f9d09e"
      },
      "source": [
        "print('Árvore de Decisão: ', round(scores_arvore.mean(),3),round(scores_arvore.std(),3))\n",
        "print('Random Forest: ', round(scores.mean(),3),round(scores.std(),3))\n",
        "print('MLP:', round(scores_mlp.mean(),3),round(scores_mlp.std(),3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Árvore de Decisão:  0.625 0.288\n",
            "Random Forest:  0.74 0.215\n",
            "MLP: 0.753 0.196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9re-mGDitRL",
        "colab_type": "text"
      },
      "source": [
        "**Comentários**\n",
        "\n",
        "*Constatou-se no tópico acima, que os dados da MLP foi acima dos demais algoritmos testados com a média de 75,3%. Portanto, pode se supor que a MLP seria mais indicada para este dataset treinado e testado.*\n",
        "\n",
        "Árvore de Decisão:  0.625 0.288\n",
        "Random Forest:  0.74 0.215\n",
        "MLP: 0.753 0.196\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr1a74wDZFRW",
        "colab_type": "text"
      },
      "source": [
        "## 8. Otimização de Parâmetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-rnBC6w4qxa",
        "colab_type": "text"
      },
      "source": [
        "## Otimizando parâmetros\n",
        "* Problema \n",
        "  * qual a melhor configuração de parâmetros para o modelo\n",
        "* Otimização\n",
        "  * escolher o melhor elemento de um conjunto\n",
        "  * o significado de melhor é dado por uma função objetivo\n",
        "    * taxa de erro\n",
        "\n",
        "  <img src=https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/ac3f2f5a-9199-4bb7-8ce6-47e4dc307a0e.png width=500>\n",
        "\n",
        "* Solução \"mais simples\"\n",
        "  * tente todas as possibilidades\n",
        "  * alto custo computacional\n",
        "* Solução heurística\n",
        "  * otimização estocástica\n",
        "  * busca no espaço de soluções\n",
        "* Random search\n",
        "  * busca aleatória\n",
        "  * sorteia alguns pontos do espaço e escolhe o melhor\n",
        "\n",
        "  <img src= https://maelfabien.github.io/assets/images/expl4_4.jpg width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq9brTrD42W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHVpcuha47Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = [\n",
        "              {\n",
        "                  'hidden_layer_sizes': [(10),(50),(100),(50,10),(100,50)],\n",
        "                  'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "                  'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "                  'max_iter': [500,1000,2000]\n",
        "              }\n",
        "              \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIjjvfLX5AGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = RandomizedSearchCV(MLPClassifier(),param_grid,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w45VEV-c5Dj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f53584-c882-475b-aad1-dd96c587d905"
      },
      "source": [
        "mlp.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                           batch_size='auto', beta_1=0.9,\n",
              "                                           beta_2=0.999, early_stopping=False,\n",
              "                                           epsilon=1e-08,\n",
              "                                           hidden_layer_sizes=(100,),\n",
              "                                           learning_rate='constant',\n",
              "                                           learning_rate_init=0.001,\n",
              "                                           max_fun=15000, max_iter=200,\n",
              "                                           momentum=0.9, n_iter_no_change=10,\n",
              "                                           nesterovs_momentum=True, power_t=0.5,\n",
              "                                           random...\n",
              "                                           verbose=False, warm_start=False),\n",
              "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
              "                   param_distributions=[{'activation': ['identity', 'logistic',\n",
              "                                                        'tanh', 'relu'],\n",
              "                                         'hidden_layer_sizes': [10, 50, 100,\n",
              "                                                                (50, 10),\n",
              "                                                                (100, 50)],\n",
              "                                         'max_iter': [500, 1000, 2000],\n",
              "                                         'solver': ['lbfgs', 'sgd', 'adam']}],\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmwklDZa5KBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2d28d24-34fb-46b0-e9e6-27e4106805ca"
      },
      "source": [
        "print(mlp.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'solver': 'sgd', 'max_iter': 500, 'hidden_layer_sizes': (100, 50), 'activation': 'logistic'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UVOxMlB5MZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1067b8ce-6d0b-423b-ece4-c6daae8df015"
      },
      "source": [
        "print(round(mlp.best_score_,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHI_cr6X5VId",
        "colab_type": "text"
      },
      "source": [
        "* Grid search\n",
        "  * monta um espaço de soluções reduzido como um reticulado\n",
        "  * testa todas as soluções, guardando a melhor\n",
        "\n",
        "  <img src=https://maelfabien.github.io/assets/images/expl4_1.jpg width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKUR7nw5dAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHjsys4L5hE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = GridSearchCV(MLPClassifier(),param_grid,cv=5,scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-wjpQL35lB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "397ded19-3d55-42dd-9a78-dee951182b3b"
      },
      "source": [
        "mlp.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=200, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_state...\n",
              "                                     solver='adam', tol=0.0001,\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'activation': ['identity', 'logistic', 'tanh',\n",
              "                                         'relu'],\n",
              "                          'hidden_layer_sizes': [10, 50, 100, (50, 10),\n",
              "                                                 (100, 50)],\n",
              "                          'max_iter': [500, 1000, 2000],\n",
              "                          'solver': ['lbfgs', 'sgd', 'adam']}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFpQWtbY5rn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913d1ae9-2a7b-4087-beab-bd508b0eebad"
      },
      "source": [
        "print(mlp.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'activation': 'logistic', 'hidden_layer_sizes': 10, 'max_iter': 500, 'solver': 'sgd'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkde78oe5vlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06be1976-2439-4cf0-ca2f-c630ea7e61b6"
      },
      "source": [
        "print(mlp.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8024950624909447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FHevLzW50Ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c8babc-ae21-4092-fd01-2ead7d4c2363"
      },
      "source": [
        "mlp.cv_results_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([1.10464430e-01, 2.19554105e+00, 1.13392153e+00, 1.29531860e-01,\n",
              "        2.07455106e+00, 1.02314172e+00, 1.05195093e-01, 2.04318895e+00,\n",
              "        1.15094166e+00, 2.39353037e-01, 3.97742710e+00, 1.44709725e+00,\n",
              "        2.39681053e-01, 3.96789680e+00, 1.47392168e+00, 2.46884441e-01,\n",
              "        4.16655841e+00, 1.45619149e+00, 4.17787313e-01, 4.91115952e+00,\n",
              "        1.53710685e+00, 4.27790213e-01, 5.02596769e+00, 1.48417602e+00,\n",
              "        4.27638102e-01, 5.00482717e+00, 1.53384337e+00, 3.45497417e-01,\n",
              "        1.92572303e+00, 7.77133179e-01, 2.84959078e-01, 2.22407160e+00,\n",
              "        8.98413467e-01, 3.63014317e-01, 1.93205800e+00, 7.39578724e-01,\n",
              "        8.29304409e-01, 4.93801293e+00, 1.33730068e+00, 9.91351509e-01,\n",
              "        5.10659690e+00, 1.37221932e+00, 9.44465351e-01, 4.93699126e+00,\n",
              "        1.52130513e+00, 1.64749379e+00, 3.27374458e-01, 2.39998279e+00,\n",
              "        3.33101387e+00, 3.12466431e-01, 2.44634371e+00, 2.74551077e+00,\n",
              "        3.03475857e-01, 2.48590837e+00, 8.24879642e+00, 5.46731853e-01,\n",
              "        4.55577068e+00, 2.03913414e+01, 4.85685444e-01, 5.14427819e+00,\n",
              "        2.46582927e+01, 4.85944366e-01, 5.16558404e+00, 1.99146036e+01,\n",
              "        5.35963249e+00, 6.07640224e+00, 3.93860642e+01, 8.89427614e-01,\n",
              "        5.38764787e+00, 6.35479209e+01, 7.25866365e-01, 5.60738645e+00,\n",
              "        1.23699622e+01, 6.03652430e-01, 4.38119054e+00, 1.99841557e+01,\n",
              "        5.93643284e-01, 4.33935685e+00, 3.98977878e+01, 5.75086069e-01,\n",
              "        4.07632484e+00, 5.11089859e+00, 1.16376319e+00, 1.78571362e+01,\n",
              "        4.03045695e+01, 1.16128974e+00, 1.88841863e+01, 1.04288592e+02,\n",
              "        1.15930376e+00, 1.86464376e+01, 2.92422533e+00, 3.01305647e+00,\n",
              "        4.15246544e+00, 3.77088227e+00, 2.84972062e+00, 3.76534286e+00,\n",
              "        5.07632837e+00, 3.06460724e+00, 4.13156676e+00, 1.29836888e+01,\n",
              "        8.22080941e+00, 4.54286790e+00, 2.62131799e+01, 8.09225502e+00,\n",
              "        7.01699805e+00, 5.26657806e+01, 7.64660420e+00, 6.53519011e+00,\n",
              "        2.42573121e+01, 1.17129591e+01, 3.85679946e+00, 4.99143321e+01,\n",
              "        1.16249520e+01, 3.08787875e+00, 9.95755516e+01, 1.20712221e+01,\n",
              "        3.55164480e+00, 1.46170492e+01, 5.60032196e+00, 4.62098398e+00,\n",
              "        2.58458850e+01, 4.87111559e+00, 4.20329638e+00, 5.90676587e+01,\n",
              "        4.76146560e+00, 4.27725244e+00, 3.82983247e+01, 1.13436445e+01,\n",
              "        1.44171143e+01, 7.45399170e+01, 1.08978095e+01, 1.04856933e+01,\n",
              "        1.44642070e+02, 1.11040850e+01, 1.44236784e+01, 1.31231260e+00,\n",
              "        3.25462909e+00, 2.00411963e+00, 2.14449530e+00, 3.19915223e+00,\n",
              "        3.12602677e+00, 2.83163896e+00, 3.37181296e+00, 2.22776608e+00,\n",
              "        4.93463879e+00, 7.41431813e+00, 5.39496460e+00, 7.94503741e+00,\n",
              "        7.47841244e+00, 4.64460626e+00, 1.96265654e+01, 6.88192487e+00,\n",
              "        4.34117079e+00, 8.98663316e+00, 1.02010082e+01, 6.99955831e+00,\n",
              "        1.79415355e+01, 9.44321423e+00, 6.71178303e+00, 3.50863707e+01,\n",
              "        9.47538605e+00, 6.23332458e+00, 6.19959726e+00, 6.42653074e+00,\n",
              "        4.44036818e+00, 1.22528350e+01, 5.11859565e+00, 4.14965920e+00,\n",
              "        2.17735321e+01, 4.94863591e+00, 5.61137905e+00, 1.64768300e+01,\n",
              "        1.31692127e+01, 1.13765446e+01, 2.64893867e+01, 1.27153668e+01,\n",
              "        1.25058694e+01, 6.49829916e+01, 1.27476110e+01, 1.31695686e+01]),\n",
              " 'mean_score_time': array([0.00216331, 0.00202274, 0.00196404, 0.00212622, 0.00202098,\n",
              "        0.00197644, 0.00214486, 0.00208035, 0.00204787, 0.00245008,\n",
              "        0.00254254, 0.00247016, 0.00245728, 0.00251856, 0.00251822,\n",
              "        0.00249338, 0.00251293, 0.00250363, 0.00291643, 0.00446463,\n",
              "        0.00293555, 0.00294943, 0.00293293, 0.00291371, 0.00290418,\n",
              "        0.00302553, 0.00294251, 0.00265074, 0.00255876, 0.00281587,\n",
              "        0.00429616, 0.00254087, 0.0025414 , 0.00268259, 0.00254087,\n",
              "        0.00252905, 0.00393138, 0.00386882, 0.00384893, 0.00393472,\n",
              "        0.00393867, 0.00392346, 0.00400352, 0.00386291, 0.00388517,\n",
              "        0.00276494, 0.00230551, 0.00234118, 0.00281229, 0.00239496,\n",
              "        0.00228052, 0.00275574, 0.00236773, 0.00232472, 0.00532856,\n",
              "        0.00535727, 0.00509043, 0.00518689, 0.00513701, 0.00512228,\n",
              "        0.00519514, 0.00520954, 0.00516105, 0.00828547, 0.00808368,\n",
              "        0.00806861, 0.00822277, 0.00815725, 0.00813351, 0.00819826,\n",
              "        0.00808649, 0.00823708, 0.00596609, 0.00582099, 0.00578785,\n",
              "        0.00614381, 0.00582252, 0.00578127, 0.00600839, 0.00582547,\n",
              "        0.00584645, 0.01168098, 0.01165004, 0.01179976, 0.01185193,\n",
              "        0.01167092, 0.01175098, 0.01183562, 0.01173282, 0.01174507,\n",
              "        0.00298028, 0.00241752, 0.00242419, 0.00295758, 0.00237179,\n",
              "        0.00248432, 0.00300164, 0.00241485, 0.00247297, 0.00631633,\n",
              "        0.00534029, 0.00537739, 0.00638523, 0.00525937, 0.00559335,\n",
              "        0.00645642, 0.00536184, 0.00548267, 0.01047049, 0.00822787,\n",
              "        0.00835094, 0.01090136, 0.00822158, 0.00843463, 0.01124806,\n",
              "        0.00823207, 0.00833783, 0.00719366, 0.0059505 , 0.00617385,\n",
              "        0.00719752, 0.00585766, 0.00605412, 0.00757856, 0.00600977,\n",
              "        0.00614061, 0.01526475, 0.01410093, 0.01250272, 0.01493635,\n",
              "        0.01151257, 0.0122025 , 0.01498256, 0.01147304, 0.01237521,\n",
              "        0.00240483, 0.00193582, 0.0019434 , 0.00223932, 0.00195622,\n",
              "        0.00200682, 0.00228915, 0.00200405, 0.00200138, 0.00320182,\n",
              "        0.00286922, 0.0028842 , 0.00319471, 0.00290165, 0.00285988,\n",
              "        0.00289841, 0.00282145, 0.00282722, 0.00366063, 0.00370798,\n",
              "        0.00372434, 0.00392714, 0.00371032, 0.00375271, 0.00417261,\n",
              "        0.00366826, 0.00372486, 0.00337019, 0.00307522, 0.00305376,\n",
              "        0.00344386, 0.00315294, 0.00304213, 0.00320139, 0.00481544,\n",
              "        0.00318198, 0.00522647, 0.00502934, 0.00515118, 0.00488658,\n",
              "        0.00494089, 0.00517693, 0.00512609, 0.00502386, 0.00520911]),\n",
              " 'mean_test_score': array([0.7170047 , 0.7130531 , 0.71823927, 0.71614005, 0.71206468,\n",
              "        0.71675817, 0.71626358, 0.71416436, 0.71786936, 0.71675772,\n",
              "        0.71268212, 0.71243413, 0.71638712, 0.71231144, 0.71725162,\n",
              "        0.71626358, 0.71367023, 0.72132752, 0.71564592, 0.71181731,\n",
              "        0.72367656, 0.71651065, 0.71317625, 0.71292773, 0.71651065,\n",
              "        0.71330001, 0.71416352, 0.71651057, 0.71959898, 0.71255736,\n",
              "        0.7077398 , 0.71428698, 0.71181594, 0.71675772, 0.71712885,\n",
              "        0.72688666, 0.7966836 , 0.71823988, 0.71231045, 0.71651065,\n",
              "        0.71984612, 0.71515323, 0.71663411, 0.71885808, 0.71132165,\n",
              "        0.66685547, 0.80249506, 0.71416306, 0.66487925, 0.80249506,\n",
              "        0.71193977, 0.66401406, 0.80249506, 0.71132188, 0.67500431,\n",
              "        0.80249506, 0.71675772, 0.6572199 , 0.80249506, 0.71786905,\n",
              "        0.65672622, 0.80249506, 0.71576945, 0.67278445, 0.80200093,\n",
              "        0.71861018, 0.66154386, 0.80249506, 0.71861018, 0.65203181,\n",
              "        0.80249506, 0.72466345, 0.66660879, 0.80249506, 0.68044441,\n",
              "        0.6557378 , 0.80249506, 0.68130915, 0.65549135, 0.80249506,\n",
              "        0.68192681, 0.73887539, 0.80249506, 0.68279154, 0.66376745,\n",
              "        0.80249506, 0.67871495, 0.65549074, 0.80249506, 0.67982675,\n",
              "        0.66524954, 0.71206415, 0.69674742, 0.66772013, 0.71527562,\n",
              "        0.69538924, 0.66475572, 0.71367   , 0.69192925, 0.66253212,\n",
              "        0.71465796, 0.72108091, 0.65524329, 0.71132302, 0.70416222,\n",
              "        0.6519079 , 0.71453488, 0.7123106 , 0.66660848, 0.71280542,\n",
              "        0.71651118, 0.65919658, 0.71206407, 0.71972205, 0.65462593,\n",
              "        0.71329986, 0.71749861, 0.66660871, 0.71453404, 0.67476151,\n",
              "        0.65771433, 0.71490449, 0.67451445, 0.65116792, 0.71342232,\n",
              "        0.67402031, 0.65697306, 0.71910469, 0.67340265, 0.64770877,\n",
              "        0.71910515, 0.67513089, 0.6486965 , 0.71836372, 0.67093184,\n",
              "        0.67747962, 0.70452825, 0.70279871, 0.66833795, 0.70934772,\n",
              "        0.69687233, 0.67105583, 0.70131868, 0.683161  , 0.6683381 ,\n",
              "        0.7055175 , 0.67710803, 0.70589161, 0.7051482 , 0.67933071,\n",
              "        0.66426113, 0.70675405, 0.67920908, 0.6598144 , 0.70601209,\n",
              "        0.67562647, 0.65870168, 0.70662976, 0.67154981, 0.65474893,\n",
              "        0.70959379, 0.67056162, 0.66129626, 0.69971267, 0.6679675 ,\n",
              "        0.6603086 , 0.70082416, 0.66833795, 0.66141979, 0.69699716,\n",
              "        0.66636149, 0.65981386, 0.70057679, 0.66562014, 0.68303861,\n",
              "        0.70082264, 0.66932629, 0.64721441, 0.69711863, 0.66179039]),\n",
              " 'param_activation': masked_array(data=['identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'identity', 'identity', 'identity',\n",
              "                    'identity', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
              "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_hidden_layer_sizes': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 50, 50, 50, 50, 50,\n",
              "                    50, 50, 50, 50, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "                    100, (50, 10), (50, 10), (50, 10), (50, 10), (50, 10),\n",
              "                    (50, 10), (50, 10), (50, 10), (50, 10), (100, 50),\n",
              "                    (100, 50), (100, 50), (100, 50), (100, 50), (100, 50),\n",
              "                    (100, 50), (100, 50), (100, 50), 10, 10, 10, 10, 10,\n",
              "                    10, 10, 10, 10, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
              "                    100, 100, 100, 100, 100, 100, 100, 100, 100, (50, 10),\n",
              "                    (50, 10), (50, 10), (50, 10), (50, 10), (50, 10),\n",
              "                    (50, 10), (50, 10), (50, 10), (100, 50), (100, 50),\n",
              "                    (100, 50), (100, 50), (100, 50), (100, 50), (100, 50),\n",
              "                    (100, 50), (100, 50), 10, 10, 10, 10, 10, 10, 10, 10,\n",
              "                    10, 50, 50, 50, 50, 50, 50, 50, 50, 50, 100, 100, 100,\n",
              "                    100, 100, 100, 100, 100, 100, (50, 10), (50, 10),\n",
              "                    (50, 10), (50, 10), (50, 10), (50, 10), (50, 10),\n",
              "                    (50, 10), (50, 10), (100, 50), (100, 50), (100, 50),\n",
              "                    (100, 50), (100, 50), (100, 50), (100, 50), (100, 50),\n",
              "                    (100, 50), 10, 10, 10, 10, 10, 10, 10, 10, 10, 50, 50,\n",
              "                    50, 50, 50, 50, 50, 50, 50, 100, 100, 100, 100, 100,\n",
              "                    100, 100, 100, 100, (50, 10), (50, 10), (50, 10),\n",
              "                    (50, 10), (50, 10), (50, 10), (50, 10), (50, 10),\n",
              "                    (50, 10), (100, 50), (100, 50), (100, 50), (100, 50),\n",
              "                    (100, 50), (100, 50), (100, 50), (100, 50), (100, 50)],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_max_iter': masked_array(data=[500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000, 500,\n",
              "                    500, 500, 1000, 1000, 1000, 2000, 2000, 2000, 500, 500,\n",
              "                    500, 1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000, 500, 500, 500,\n",
              "                    1000, 1000, 1000, 2000, 2000, 2000],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_solver': masked_array(data=['lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam',\n",
              "                    'lbfgs', 'sgd', 'adam', 'lbfgs', 'sgd', 'adam'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'identity',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'logistic',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 10,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 50,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': 100,\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (50, 10),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 500,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 1000,\n",
              "   'solver': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'lbfgs'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'hidden_layer_sizes': (100, 50),\n",
              "   'max_iter': 2000,\n",
              "   'solver': 'adam'}],\n",
              " 'rank_test_score': array([ 39,  72,  33,  53,  81,  40,  51,  63,  34,  41,  75,  77,  50,\n",
              "         78,  37,  51,  66,  21,  55,  85,  20,  46,  71,  73,  46,  69,\n",
              "         64,  49,  25,  76,  92,  62,  86,  41,  38,  18,  16,  32,  80,\n",
              "         46,  23,  57,  44,  28,  89, 143,   1,  65, 150,   1,  84, 153,\n",
              "          1,  88, 127,   1,  41, 166,   1,  35, 168,   1,  54, 132,  15,\n",
              "         29, 157,   1,  30, 175,   1,  19, 144,   1, 118, 169,   1, 117,\n",
              "        170,   1, 116,  17,   1, 115, 154,   1, 122, 171,   1, 119, 149,\n",
              "         82, 110, 142,  56, 111, 151,  67, 112, 155,  59,  22, 172,  87,\n",
              "        100, 176,  60,  79, 146,  74,  45, 163,  83,  24, 174,  70,  36,\n",
              "        145,  61, 128, 165,  58, 129, 177,  68, 130, 167,  27, 131, 179,\n",
              "         26, 126, 178,  31, 135, 123,  99, 101, 139,  91, 109, 134, 102,\n",
              "        113, 138,  97, 124,  96,  98, 120, 152,  93, 121, 161,  95, 125,\n",
              "        164,  94, 133, 173,  90, 136, 159, 106, 141, 160, 103, 139, 158,\n",
              "        108, 147, 162, 105, 148, 114, 104, 137, 180, 107, 156], dtype=int32),\n",
              " 'split0_test_score': array([0.84691358, 0.83518519, 0.85308642, 0.8462963 , 0.83641975,\n",
              "        0.84259259, 0.8462963 , 0.83950617, 0.84753086, 0.8462963 ,\n",
              "        0.8382716 , 0.84567901, 0.8462963 , 0.83888889, 0.84814815,\n",
              "        0.8462963 , 0.83950617, 0.8537037 , 0.8462963 , 0.83888889,\n",
              "        0.8382716 , 0.8462963 , 0.8382716 , 0.85      , 0.8462963 ,\n",
              "        0.83641975, 0.8462963 , 0.84691358, 0.8462963 , 0.84814815,\n",
              "        0.8462963 , 0.84691358, 0.85      , 0.8462963 , 0.84197531,\n",
              "        0.85246914, 0.8462963 , 0.84814815, 0.84691358, 0.8462963 ,\n",
              "        0.84567901, 0.8345679 , 0.84691358, 0.84382716, 0.85123457,\n",
              "        0.80493827, 0.80246914, 0.85      , 0.80246914, 0.80246914,\n",
              "        0.84753086, 0.80617284, 0.80246914, 0.84938272, 0.84012346,\n",
              "        0.80246914, 0.8462963 , 0.80493827, 0.80246914, 0.85      ,\n",
              "        0.80123457, 0.80246914, 0.8462963 , 0.80987654, 0.80246914,\n",
              "        0.85061728, 0.80246914, 0.80246914, 0.85061728, 0.80246914,\n",
              "        0.80246914, 0.84938272, 0.80185185, 0.80246914, 0.80246914,\n",
              "        0.80246914, 0.80246914, 0.80246914, 0.79753086, 0.80246914,\n",
              "        0.80246914, 0.8037037 , 0.80246914, 0.80246914, 0.80246914,\n",
              "        0.80246914, 0.80246914, 0.80246914, 0.80246914, 0.80246914,\n",
              "        0.80493827, 0.84074074, 0.82962963, 0.80555556, 0.84382716,\n",
              "        0.82407407, 0.80246914, 0.84135802, 0.83271605, 0.80246914,\n",
              "        0.84382716, 0.85      , 0.80555556, 0.84012346, 0.80679012,\n",
              "        0.80555556, 0.84012346, 0.84567901, 0.80432099, 0.84012346,\n",
              "        0.84197531, 0.8037037 , 0.84135802, 0.85      , 0.80308642,\n",
              "        0.83765432, 0.84876543, 0.80246914, 0.84691358, 0.80555556,\n",
              "        0.80246914, 0.84814815, 0.80555556, 0.79567901, 0.8462963 ,\n",
              "        0.80555556, 0.80308642, 0.84753086, 0.80555556, 0.79753086,\n",
              "        0.84382716, 0.8154321 , 0.80185185, 0.84567901, 0.80679012,\n",
              "        0.80246914, 0.84382716, 0.84444444, 0.80432099, 0.83024691,\n",
              "        0.81851852, 0.80308642, 0.82530864, 0.8117284 , 0.80308642,\n",
              "        0.83580247, 0.81049383, 0.80740741, 0.82530864, 0.81790123,\n",
              "        0.80617284, 0.82592593, 0.80246914, 0.80246914, 0.83209877,\n",
              "        0.8037037 , 0.80987654, 0.83209877, 0.80432099, 0.80740741,\n",
              "        0.8382716 , 0.8037037 , 0.80679012, 0.82592593, 0.80308642,\n",
              "        0.80185185, 0.82839506, 0.80432099, 0.80679012, 0.80802469,\n",
              "        0.8037037 , 0.80679012, 0.8308642 , 0.80493827, 0.80246914,\n",
              "        0.84074074, 0.8037037 , 0.79938272, 0.82469136, 0.80679012]),\n",
              " 'split1_test_score': array([0.84743669, 0.83261272, 0.8437307 , 0.84620136, 0.8295244 ,\n",
              "        0.84002471, 0.84620136, 0.84126004, 0.84743669, 0.84681902,\n",
              "        0.83384805, 0.83075973, 0.84558369, 0.8276714 , 0.84743669,\n",
              "        0.84743669, 0.83384805, 0.8418777 , 0.84558369, 0.83014206,\n",
              "        0.85546634, 0.84558369, 0.83323039, 0.83755405, 0.84681902,\n",
              "        0.83137739, 0.83693638, 0.84681902, 0.8437307 , 0.83199506,\n",
              "        0.84681902, 0.83755405, 0.83570105, 0.84681902, 0.84064237,\n",
              "        0.84002471, 0.84620136, 0.84311303, 0.8418777 , 0.84681902,\n",
              "        0.84990735, 0.84990735, 0.84681902, 0.84126004, 0.82520074,\n",
              "        0.80358246, 0.80296479, 0.84496603, 0.80728845, 0.80296479,\n",
              "        0.84558369, 0.80543545, 0.80296479, 0.83940704, 0.81222977,\n",
              "        0.80296479, 0.84558369, 0.80049413, 0.80296479, 0.84681902,\n",
              "        0.81778876, 0.80296479, 0.8437307 , 0.81408277, 0.80296479,\n",
              "        0.84558369, 0.80605312, 0.80296479, 0.84064237, 0.80234713,\n",
              "        0.80296479, 0.85978999, 0.8011118 , 0.80296479, 0.80296479,\n",
              "        0.7992588 , 0.80296479, 0.80296479, 0.80234713, 0.80296479,\n",
              "        0.80296479, 0.85423101, 0.80296479, 0.80667078, 0.80420012,\n",
              "        0.80296479, 0.80420012, 0.79617048, 0.80296479, 0.80914145,\n",
              "        0.80296479, 0.83570105, 0.8153181 , 0.80420012, 0.83323039,\n",
              "        0.8153181 , 0.79987647, 0.83323039, 0.81593576, 0.80420012,\n",
              "        0.83323039, 0.84990735, 0.80481779, 0.82581841, 0.85546634,\n",
              "        0.79678814, 0.83508338, 0.86287832, 0.81284744, 0.82890673,\n",
              "        0.84990735, 0.80790611, 0.83014206, 0.84990735, 0.80790611,\n",
              "        0.83323039, 0.83878938, 0.80172946, 0.84558369, 0.81037678,\n",
              "        0.80296479, 0.84434836, 0.81099444, 0.80049413, 0.84002471,\n",
              "        0.80667078, 0.80296479, 0.84743669, 0.80605312, 0.79802347,\n",
              "        0.84805435, 0.80296479, 0.79493515, 0.8437307 , 0.80420012,\n",
              "        0.80914145, 0.80852378, 0.81099444, 0.80296479, 0.82520074,\n",
              "        0.84126004, 0.80852378, 0.83570105, 0.8153181 , 0.81284744,\n",
              "        0.81717109, 0.81222977, 0.80543545, 0.82520074, 0.81778876,\n",
              "        0.80728845, 0.8295244 , 0.80852378, 0.80481779, 0.82828907,\n",
              "        0.80605312, 0.80234713, 0.8295244 , 0.80852378, 0.80296479,\n",
              "        0.82828907, 0.80481779, 0.80358246, 0.81470043, 0.81161211,\n",
              "        0.80234713, 0.8153181 , 0.80420012, 0.80358246, 0.83446572,\n",
              "        0.80296479, 0.80296479, 0.83014206, 0.81037678, 0.8011118 ,\n",
              "        0.83878938, 0.81717109, 0.7992588 , 0.82458308, 0.8134651 ]),\n",
              " 'split2_test_score': array([0.85546634, 0.8295244 , 0.85176035, 0.85484867, 0.83014206,\n",
              "        0.856084  , 0.85484867, 0.82520074, 0.85361334, 0.85546634,\n",
              "        0.82643607, 0.85546634, 0.85546634, 0.83014206, 0.85423101,\n",
              "        0.85423101, 0.82705374, 0.87955528, 0.85299568, 0.82643607,\n",
              "        0.86843731, 0.856084  , 0.83075973, 0.84867202, 0.85484867,\n",
              "        0.83014206, 0.85052502, 0.85484867, 0.84928968, 0.85546634,\n",
              "        0.81161211, 0.84002471, 0.84928968, 0.85731933, 0.85299568,\n",
              "        0.88202594, 0.85423101, 0.85114268, 0.83755405, 0.85546634,\n",
              "        0.84928968, 0.84434836, 0.856084  , 0.85176035, 0.82643607,\n",
              "        0.80543545, 0.80234713, 0.857937  , 0.80234713, 0.80234713,\n",
              "        0.8437307 , 0.79802347, 0.80234713, 0.84743669, 0.80790611,\n",
              "        0.80234713, 0.85670167, 0.80234713, 0.80234713, 0.85176035,\n",
              "        0.81655343, 0.80234713, 0.85917233, 0.81037678, 0.80234713,\n",
              "        0.86164299, 0.80667078, 0.80234713, 0.86596665, 0.80543545,\n",
              "        0.80234713, 0.86967264, 0.80172946, 0.80234713, 0.80234713,\n",
              "        0.80234713, 0.80234713, 0.80234713, 0.80172946, 0.80234713,\n",
              "        0.80234713, 0.80234713, 0.80234713, 0.80234713, 0.80049413,\n",
              "        0.80234713, 0.80234713, 0.80605312, 0.80234713, 0.80234713,\n",
              "        0.80296479, 0.82828907, 0.8437307 , 0.80234713, 0.84249537,\n",
              "        0.84064237, 0.80358246, 0.83446572, 0.82149475, 0.80172946,\n",
              "        0.83075973, 0.88202594, 0.79987647, 0.83137739, 0.85670167,\n",
              "        0.8011118 , 0.83261272, 0.86781964, 0.80481779, 0.83014206,\n",
              "        0.85423101, 0.80420012, 0.82643607, 0.857937  , 0.80481779,\n",
              "        0.82828907, 0.86287832, 0.80172946, 0.85731933, 0.80296479,\n",
              "        0.80234713, 0.85484867, 0.80234713, 0.79555281, 0.85299568,\n",
              "        0.80234713, 0.80049413, 0.85114268, 0.80481779, 0.8011118 ,\n",
              "        0.84928968, 0.80296479, 0.80049413, 0.85237801, 0.80234713,\n",
              "        0.80420012, 0.8276714 , 0.85299568, 0.80358246, 0.83261272,\n",
              "        0.85670167, 0.79987647, 0.80234713, 0.8153181 , 0.81284744,\n",
              "        0.83631872, 0.80728845, 0.79987647, 0.83199506, 0.80543545,\n",
              "        0.80543545, 0.82643607, 0.81222977, 0.80790611, 0.83261272,\n",
              "        0.82643607, 0.80172946, 0.82334775, 0.80358246, 0.80358246,\n",
              "        0.83014206, 0.80358246, 0.80420012, 0.84928968, 0.80234713,\n",
              "        0.80358246, 0.85299568, 0.80481779, 0.80049413, 0.84805435,\n",
              "        0.80234713, 0.80420012, 0.84064237, 0.80481779, 0.80358246,\n",
              "        0.81840642, 0.80296479, 0.8011118 , 0.83137739, 0.80358246]),\n",
              " 'split3_test_score': array([0.75355158, 0.76281655, 0.75416924, 0.75293391, 0.76096356,\n",
              "        0.75416924, 0.75293391, 0.76158122, 0.75293391, 0.75355158,\n",
              "        0.76096356, 0.75169858, 0.75355158, 0.75787523, 0.75108091,\n",
              "        0.75293391, 0.76096356, 0.75478691, 0.75293391, 0.75972823,\n",
              "        0.7584929 , 0.75355158, 0.75725757, 0.75169858, 0.75355158,\n",
              "        0.76343422, 0.75231624, 0.75293391, 0.76034589, 0.75231624,\n",
              "        0.75293391, 0.75478691, 0.75293391, 0.75293391, 0.75602224,\n",
              "        0.75787523, 0.75231624, 0.75540457, 0.75911056, 0.75293391,\n",
              "        0.76219889, 0.75355158, 0.75293391, 0.76158122, 0.75478691,\n",
              "        0.70105003, 0.80234713, 0.75787523, 0.69302038, 0.80234713,\n",
              "        0.7566399 , 0.69425571, 0.80234713, 0.75540457, 0.69734404,\n",
              "        0.80234713, 0.75725757, 0.67078443, 0.80234713, 0.75602224,\n",
              "        0.64793082, 0.80234713, 0.75169858, 0.71155034, 0.80234713,\n",
              "        0.75602224, 0.68128474, 0.80234713, 0.75602224, 0.65040148,\n",
              "        0.80234713, 0.75787523, 0.71525633, 0.80234713, 0.75540457,\n",
              "        0.66707844, 0.80234713, 0.76034589, 0.66151946, 0.80234713,\n",
              "        0.76343422, 0.80420012, 0.80234713, 0.76096356, 0.70352069,\n",
              "        0.80234713, 0.75355158, 0.6714021 , 0.80234713, 0.75725757,\n",
              "        0.7016677 , 0.76096356, 0.74490426, 0.70969734, 0.76034589,\n",
              "        0.74552193, 0.69857937, 0.76281655, 0.74675726, 0.68869673,\n",
              "        0.76281655, 0.7566399 , 0.6590488 , 0.75725757, 0.75416924,\n",
              "        0.65472514, 0.75972823, 0.75231624, 0.69487338, 0.75972823,\n",
              "        0.75355158, 0.66707844, 0.7584929 , 0.76219889, 0.65472514,\n",
              "        0.76219889, 0.75293391, 0.71340334, 0.75231624, 0.73193329,\n",
              "        0.68190241, 0.75046325, 0.73378629, 0.66337245, 0.75355158,\n",
              "        0.73502162, 0.67634342, 0.75787523, 0.73193329, 0.64360716,\n",
              "        0.76034589, 0.72946263, 0.64854849, 0.7584929 , 0.7300803 ,\n",
              "        0.75293391, 0.76158122, 0.75478691, 0.69610871, 0.76096356,\n",
              "        0.73193329, 0.71278567, 0.76466955, 0.73810994, 0.69240272,\n",
              "        0.75478691, 0.72760964, 0.67201977, 0.76590488, 0.72266831,\n",
              "        0.68252007, 0.75787523, 0.73502162, 0.6695491 , 0.75972823,\n",
              "        0.72019765, 0.6714021 , 0.76158122, 0.72266831, 0.65596047,\n",
              "        0.76281655, 0.71710933, 0.68128474, 0.75046325, 0.70846201,\n",
              "        0.68066708, 0.74675726, 0.71093267, 0.68004941, 0.7442866 ,\n",
              "        0.71093267, 0.67263743, 0.74366893, 0.68622607, 0.80234713,\n",
              "        0.75478691, 0.70599135, 0.63434219, 0.74861025, 0.6714021 ]),\n",
              " 'split4_test_score': array([0.28165534, 0.30512662, 0.28844966, 0.28042001, 0.30327363,\n",
              "        0.29092032, 0.28103768, 0.30327363, 0.287832  , 0.28165534,\n",
              "        0.30389129, 0.27856702, 0.28103768, 0.30697962, 0.28536133,\n",
              "        0.28042001, 0.30697962, 0.27671402, 0.28042001, 0.30389129,\n",
              "        0.29771464, 0.28103768, 0.30636195, 0.27671402, 0.28103768,\n",
              "        0.30512662, 0.28474367, 0.28103768, 0.2983323 , 0.27486103,\n",
              "        0.28103768, 0.29215565, 0.27115503, 0.28042001, 0.29400865,\n",
              "        0.3020383 , 0.68437307, 0.29339098, 0.27609636, 0.28103768,\n",
              "        0.29215565, 0.29339098, 0.28042001, 0.29586164, 0.29894997,\n",
              "        0.21927116, 0.80234713, 0.26003706, 0.21927116, 0.80234713,\n",
              "        0.26621371, 0.21618283, 0.80234713, 0.26497838, 0.21741816,\n",
              "        0.80234713, 0.27794935, 0.20753552, 0.80234713, 0.28474367,\n",
              "        0.20012353, 0.80234713, 0.27794935, 0.21803582, 0.79987647,\n",
              "        0.27918468, 0.21124151, 0.80234713, 0.27980235, 0.19950587,\n",
              "        0.80234713, 0.28659666, 0.2130945 , 0.80234713, 0.23903644,\n",
              "        0.20753552, 0.80234713, 0.23841878, 0.21432983, 0.80234713,\n",
              "        0.23841878, 0.429895  , 0.80234713, 0.2415071 , 0.20815318,\n",
              "        0.80234713, 0.23100679, 0.20135886, 0.80234713, 0.22791847,\n",
              "        0.21371217, 0.29462631, 0.25015442, 0.21680049, 0.29647931,\n",
              "        0.25138975, 0.21927116, 0.29647931, 0.24274243, 0.21556516,\n",
              "        0.30265596, 0.26683138, 0.20691785, 0.3020383 , 0.24768376,\n",
              "        0.20135886, 0.30512662, 0.23285979, 0.21618283, 0.30512662,\n",
              "        0.28289067, 0.2130945 , 0.30389129, 0.27856702, 0.20259419,\n",
              "        0.30512662, 0.284126  , 0.21371217, 0.27053737, 0.22297715,\n",
              "        0.1988882 , 0.27671402, 0.21988882, 0.2007412 , 0.27424336,\n",
              "        0.22050649, 0.20197653, 0.29153799, 0.21865349, 0.19827054,\n",
              "        0.29400865, 0.22483014, 0.19765287, 0.29153799, 0.21124151,\n",
              "        0.21865349, 0.28103768, 0.25077208, 0.23471279, 0.29771464,\n",
              "        0.23594812, 0.23100679, 0.27856702, 0.23533045, 0.22050649,\n",
              "        0.28350834, 0.22791847, 0.44471896, 0.27733169, 0.23285979,\n",
              "        0.21988882, 0.29400865, 0.23780111, 0.21432983, 0.27733169,\n",
              "        0.22174182, 0.20815318, 0.28659666, 0.21865349, 0.20382952,\n",
              "        0.28844966, 0.22359481, 0.21062384, 0.25818406, 0.21432983,\n",
              "        0.2130945 , 0.26065473, 0.21741816, 0.21618283, 0.25015442,\n",
              "        0.21185917, 0.21247684, 0.2575664 , 0.22174182, 0.20568252,\n",
              "        0.25138975, 0.21680049, 0.20197653, 0.25633107, 0.21371217]),\n",
              " 'std_fit_time': array([3.93308749e-02, 2.18639993e-01, 3.71777729e-01, 1.38034651e-02,\n",
              "        1.40244298e-01, 1.81076409e-01, 1.01270706e-02, 1.85795660e-01,\n",
              "        3.02938747e-01, 3.10771150e-02, 3.22289837e-01, 2.93739178e-01,\n",
              "        2.58632144e-02, 2.61612296e-01, 1.71383191e-01, 1.61016703e-02,\n",
              "        2.56495216e-01, 3.42574763e-01, 4.75938118e-02, 4.43289293e-01,\n",
              "        2.49085554e-01, 5.11691057e-02, 2.71592182e-01, 2.14045022e-01,\n",
              "        1.19242096e-01, 3.51867533e-01, 1.84157128e-01, 2.86867475e-02,\n",
              "        1.66574603e-01, 2.62302538e-01, 1.04631769e-01, 8.66100282e-01,\n",
              "        1.41933699e-01, 1.98317878e-02, 1.30935005e-01, 1.12058471e-01,\n",
              "        3.14836445e-01, 2.33704451e-01, 1.35950394e-01, 1.58691678e-01,\n",
              "        6.10902905e-01, 2.57396840e-01, 9.66857260e-02, 1.88557829e-01,\n",
              "        4.39161252e-01, 5.45477054e-01, 5.01112587e-02, 3.88210757e-01,\n",
              "        1.28524226e+00, 2.06404071e-02, 2.83336561e-01, 8.24430539e-01,\n",
              "        1.54711380e-02, 5.01961497e-01, 3.98923394e+00, 1.05648830e-01,\n",
              "        9.44711119e-01, 1.39831831e-01, 1.75244296e-02, 8.87789608e-01,\n",
              "        1.97473388e+01, 1.91469294e-02, 1.81659810e+00, 1.76088534e-01,\n",
              "        8.82081369e+00, 1.58460861e+00, 7.43693430e-01, 3.62907179e-01,\n",
              "        1.18888637e+00, 3.14380038e+01, 1.45365622e-01, 1.35507523e+00,\n",
              "        1.24985770e-01, 3.53774938e-02, 2.35474194e-01, 5.27924571e+00,\n",
              "        2.94587705e-02, 5.09139043e-01, 1.11458356e+01, 6.46992250e-02,\n",
              "        5.42213252e-01, 4.74514023e+00, 4.74462964e-02, 2.91309897e+00,\n",
              "        3.19422791e+01, 1.30898347e-02, 3.76134247e+00, 5.16549518e+01,\n",
              "        1.02794404e-02, 3.80053995e+00, 6.47121334e-01, 3.29616862e-01,\n",
              "        4.63432038e-01, 2.14797771e+00, 2.88356213e-01, 7.73214848e-01,\n",
              "        4.21380112e+00, 1.44490524e-01, 5.66298250e-01, 1.59827778e-01,\n",
              "        5.07630397e-01, 3.33617341e+00, 4.15281073e-01, 2.41374359e-01,\n",
              "        5.51072609e+00, 5.62877684e-01, 6.47065552e-01, 6.46443107e+00,\n",
              "        6.25003365e-01, 9.93388094e-01, 9.14786282e-01, 1.25739478e+00,\n",
              "        6.91730230e-01, 5.91938867e-01, 1.26621091e+00, 5.18340772e-01,\n",
              "        8.57718144e-01, 3.05182885e-01, 1.29826272e+00, 1.64535560e+00,\n",
              "        7.15314626e+00, 1.00210815e+00, 9.47191615e-01, 2.82254804e-01,\n",
              "        8.14167178e-01, 1.71539222e+00, 7.96106375e-01, 4.49378079e-01,\n",
              "        1.49920942e+01, 1.50083505e+00, 7.00272145e-01, 5.29398826e+00,\n",
              "        7.26351330e+00, 7.04449437e-01, 1.09623982e+01, 7.98427494e-01,\n",
              "        4.57032507e-01, 8.98624124e-01, 1.34102313e+00, 3.64016767e-01,\n",
              "        1.00921708e+00, 1.83709284e+00, 4.32714666e-01, 3.24361500e-01,\n",
              "        8.00012058e-02, 4.74435774e-01, 1.96105236e+00, 3.93992076e+00,\n",
              "        5.84934205e-01, 1.08096568e+00, 1.09026963e+00, 6.04958354e-01,\n",
              "        5.97065015e-01, 3.01025332e-01, 6.45993573e-01, 2.74097659e+00,\n",
              "        3.85593751e-01, 1.17199483e+00, 3.43857478e+00, 7.83868979e-01,\n",
              "        3.23747544e-01, 1.73006977e+00, 1.18390944e-01, 1.23133132e+00,\n",
              "        1.56821490e+00, 3.35961090e-01, 1.25095663e+00, 1.71059528e+00,\n",
              "        6.34007640e+00, 4.27178324e-01, 1.74747517e+00, 9.28011485e-02,\n",
              "        1.67675647e+00, 3.66921775e+00, 1.31125927e+01, 1.38035449e+00,\n",
              "        1.97431445e+00, 1.28412581e+00, 1.51472656e+00, 4.63491543e+00]),\n",
              " 'std_score_time': array([4.05590859e-05, 5.51509123e-05, 8.15154482e-06, 1.61513411e-05,\n",
              "        8.14745184e-05, 4.56872211e-05, 2.77285625e-05, 9.48453680e-05,\n",
              "        6.31489043e-05, 2.68225034e-05, 6.25246803e-05, 2.53615515e-05,\n",
              "        1.05859997e-05, 5.54569200e-05, 7.52121101e-05, 5.40595399e-05,\n",
              "        7.16553147e-05, 4.53895377e-05, 2.35335442e-05, 2.98559572e-03,\n",
              "        6.51457292e-05, 6.71913133e-05, 2.81610999e-05, 5.64209394e-05,\n",
              "        1.90394814e-05, 9.50005149e-05, 1.11356271e-04, 2.85246682e-05,\n",
              "        7.03475259e-05, 5.43334414e-04, 2.99600528e-03, 6.40937966e-05,\n",
              "        2.50003680e-05, 1.44542234e-05, 4.95328929e-05, 8.32791148e-05,\n",
              "        4.37035238e-05, 7.45734289e-05, 6.75783741e-05, 9.75592511e-05,\n",
              "        5.47397843e-05, 1.54520212e-04, 1.00487930e-04, 6.58714593e-05,\n",
              "        4.21780189e-05, 6.67064337e-05, 5.54418299e-05, 1.26074753e-04,\n",
              "        6.72175678e-05, 1.40784215e-04, 1.24356717e-05, 5.81519679e-05,\n",
              "        9.57294982e-05, 6.14699508e-05, 5.61080439e-04, 5.43318645e-04,\n",
              "        4.45319445e-05, 1.60545668e-04, 8.78904667e-05, 6.51134365e-05,\n",
              "        1.66060414e-04, 1.50396632e-04, 4.04842891e-05, 2.23185739e-04,\n",
              "        5.75300820e-05, 2.88395434e-05, 1.82211664e-04, 8.99608621e-05,\n",
              "        2.45456326e-04, 1.69701247e-04, 4.20475752e-05, 1.41627973e-04,\n",
              "        6.31078441e-05, 4.32354854e-05, 4.29730557e-05, 2.03222436e-04,\n",
              "        7.30430795e-05, 7.85629701e-05, 1.11206359e-04, 4.63350791e-05,\n",
              "        9.25368082e-05, 2.40401604e-05, 2.35971288e-05, 7.43053065e-05,\n",
              "        1.27059039e-04, 3.71451555e-05, 3.10952673e-05, 1.46259443e-04,\n",
              "        1.11067545e-04, 1.25380847e-04, 1.65562091e-05, 2.47475269e-05,\n",
              "        2.04169845e-05, 4.62664752e-05, 3.33918818e-05, 5.86143240e-05,\n",
              "        2.89567781e-05, 4.19458446e-05, 3.15328198e-05, 1.34703974e-04,\n",
              "        4.92901568e-05, 1.70051162e-04, 9.64579931e-05, 3.95842627e-05,\n",
              "        2.99832096e-04, 9.01201288e-05, 2.59207267e-04, 2.91117852e-04,\n",
              "        3.84547945e-04, 9.04582083e-05, 1.01828304e-04, 2.68166732e-04,\n",
              "        9.05256727e-05, 1.22291171e-04, 5.50806311e-04, 1.30030214e-04,\n",
              "        9.35938666e-05, 2.97326900e-04, 4.26329221e-05, 7.79205026e-05,\n",
              "        5.13599942e-04, 5.28247340e-05, 5.83176718e-05, 1.94243080e-04,\n",
              "        6.63771026e-05, 1.17298995e-04, 2.39583270e-04, 4.65646645e-03,\n",
              "        4.19225169e-04, 2.08489794e-04, 7.40863475e-05, 3.09106870e-04,\n",
              "        2.48801484e-04, 5.57490957e-05, 3.25914957e-04, 1.92850102e-04,\n",
              "        1.59947418e-05, 2.80959478e-05, 3.43235323e-05, 3.23283637e-05,\n",
              "        6.36864894e-05, 6.53111307e-05, 5.51341302e-05, 5.18251140e-05,\n",
              "        8.57944677e-05, 6.73625273e-05, 8.86740718e-05, 2.15769768e-04,\n",
              "        8.29563185e-05, 5.03684063e-05, 1.58642110e-04, 3.53840804e-05,\n",
              "        9.56789413e-05, 1.98790340e-04, 1.13225744e-04, 1.04114639e-04,\n",
              "        3.33149391e-04, 6.27542701e-05, 1.42141710e-04, 8.43188330e-05,\n",
              "        7.92470362e-05, 7.77145684e-05, 1.26951802e-04, 7.22644471e-05,\n",
              "        7.36142372e-05, 2.17989673e-04, 1.08800050e-04, 7.39615167e-05,\n",
              "        1.36460398e-04, 3.59996405e-03, 5.01716538e-05, 1.76200047e-04,\n",
              "        2.06947964e-04, 1.07702368e-04, 2.63378189e-05, 8.71393781e-05,\n",
              "        1.03098501e-04, 1.41874529e-04, 1.24832981e-04, 6.00691056e-05]),\n",
              " 'std_test_score': array([2.20873350e-01, 2.05745827e-01, 2.18068705e-01, 2.21044074e-01,\n",
              "        2.06254437e-01, 2.15952814e-01, 2.20800571e-01, 2.07495790e-01,\n",
              "        2.18260342e-01, 2.20727947e-01, 2.06317669e-01, 2.19998058e-01,\n",
              "        2.20826407e-01, 2.04735817e-01, 2.19325806e-01, 2.21112746e-01,\n",
              "        2.05312994e-01, 2.26226665e-01, 2.20739795e-01, 2.05905069e-01,\n",
              "        2.16384833e-01, 2.20904335e-01, 2.05586359e-01, 2.21148193e-01,\n",
              "        2.20893972e-01, 2.05850314e-01, 2.17707943e-01, 2.20946133e-01,\n",
              "        2.13263450e-01, 2.21915322e-01, 2.16077578e-01, 2.13742678e-01,\n",
              "        2.23255059e-01, 2.21428573e-01, 2.14404022e-01, 2.16377354e-01,\n",
              "        6.75385068e-02, 2.15411428e-01, 2.20483387e-01, 2.20950960e-01,\n",
              "        2.16433995e-01, 2.13760495e-01, 2.21344771e-01, 2.14015782e-01,\n",
              "        2.08679196e-01, 2.27361619e-01, 2.39571697e-04, 2.29944869e-01,\n",
              "        2.26921692e-01, 2.39571697e-04, 2.25514661e-01, 2.27874975e-01,\n",
              "        2.39571697e-04, 2.25902477e-01, 2.33941103e-01, 2.39571697e-04,\n",
              "        2.22330252e-01, 2.30568939e-01, 2.39571697e-04, 2.19575419e-01,\n",
              "        2.37036669e-01, 2.39571697e-04, 2.22239990e-01, 2.30647008e-01,\n",
              "        1.08639752e-03, 2.22935400e-01, 2.30202778e-01, 2.39571697e-04,\n",
              "        2.22703227e-01, 2.33897904e-01, 2.39571697e-04, 2.22641988e-01,\n",
              "        2.29207824e-01, 2.39571697e-04, 2.21459509e-01, 2.30059364e-01,\n",
              "        2.39571697e-04, 2.22048967e-01, 2.27062657e-01, 2.39571697e-04,\n",
              "        2.22272145e-01, 1.55739843e-01, 2.39571697e-04, 2.21271395e-01,\n",
              "        2.31005784e-01, 2.39571697e-04, 2.24672943e-01, 2.32616164e-01,\n",
              "        2.39571697e-04, 2.26711850e-01, 2.29196948e-01, 2.10711879e-01,\n",
              "        2.25869504e-01, 2.28403339e-01, 2.11681539e-01, 2.24361327e-01,\n",
              "        2.26316593e-01, 2.10548798e-01, 2.26610089e-01, 2.27812264e-01,\n",
              "        2.07984914e-01, 2.30966130e-01, 2.31039064e-01, 2.06753847e-01,\n",
              "        2.31329633e-01, 2.32319633e-01, 2.06835379e-01, 2.43357500e-01,\n",
              "        2.29405559e-01, 2.05845106e-01, 2.19955119e-01, 2.29387031e-01,\n",
              "        2.06155741e-01, 2.23358892e-01, 2.33420490e-01, 2.05944842e-01,\n",
              "        2.20065974e-01, 2.29032009e-01, 2.25231673e-01, 2.27733348e-01,\n",
              "        2.34126793e-01, 2.22427190e-01, 2.29057795e-01, 2.31111208e-01,\n",
              "        2.22554665e-01, 2.28368733e-01, 2.32662009e-01, 2.16662344e-01,\n",
              "        2.29151963e-01, 2.32630697e-01, 2.15192623e-01, 2.27196014e-01,\n",
              "        2.32948578e-01, 2.16183464e-01, 2.31646985e-01, 2.30317203e-01,\n",
              "        2.13533510e-01, 2.28630621e-01, 2.20775383e-01, 2.07527727e-01,\n",
              "        2.34465957e-01, 2.22849188e-01, 2.12777433e-01, 2.25846036e-01,\n",
              "        2.28497382e-01, 2.13105074e-01, 2.26856021e-01, 1.40289730e-01,\n",
              "        2.15248514e-01, 2.26048339e-01, 2.27299835e-01, 2.08120361e-01,\n",
              "        2.22515560e-01, 2.28848761e-01, 2.16115498e-01, 2.29850514e-01,\n",
              "        2.31127693e-01, 2.11620470e-01, 2.28714168e-01, 2.32703137e-01,\n",
              "        2.12308019e-01, 2.26005351e-01, 2.30365149e-01, 2.23180817e-01,\n",
              "        2.29945958e-01, 2.28539603e-01, 2.22889366e-01, 2.28350446e-01,\n",
              "        2.27713612e-01, 2.26251065e-01, 2.30032340e-01, 2.29441261e-01,\n",
              "        2.24274597e-01, 2.26800609e-01, 2.38679327e-01, 2.26865559e-01,\n",
              "        2.29738350e-01, 2.31671993e-01, 2.22482567e-01, 2.30217988e-01])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJhb7-kMj5lj",
        "colab_type": "text"
      },
      "source": [
        "**Comentários**\n",
        "\n",
        "*Observou-se no tópico acima que a otimização valeu a pena...pois ambas (Random Search e GridSearch) foram acima de 80%, enquanto que a MLP padrão obteve apenas 75,3%.*"
      ]
    }
  ]
}